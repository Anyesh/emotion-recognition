{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "BASEDIR = os.path.dirname(os.path.dirname(os.path.abspath('README.md')))\n",
    "DATAPATH = os.path.join(BASEDIR, 'data', \"raw\")\n",
    "CHECKPOINT_PATH = os.path.join(BASEDIR, 'checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>emotions</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>disgust</td>\n",
       "      <td>At a gathering I found myself involuntarily si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # emotions                                              texts\n",
       "0  0      joy  On days when I feel close to my partner and ot...\n",
       "1  1     fear  Every time I imagine that someone I love or I ...\n",
       "2  2    anger  When I had been obviously unjustly treated and...\n",
       "3  3  sadness  When I think about the short time that we live...\n",
       "4  4  disgust  At a gathering I found myself involuntarily si..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATAPATH, 'ISEAR_dataset.csv'), names=['#', 'emotions', 'texts'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, random_state=64, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_data['e'] = encoder.fit_transform(train_data['emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [0,1,2,3,4,5,6]\n",
    "mapper = dict(zip( values, encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapper =  dict(zip( encoder.classes_, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'guilt': 3,\n",
       " 'joy': 4,\n",
       " 'sadness': 5,\n",
       " 'shame': 6}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = train_data[[ 'e', 'texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_new_lines(text):\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.replace('\\t', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_df['texts'] = new_df['texts'].apply(remove_new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5693    When I dated my friend's boyfriend after they ...\n",
       "6457    When I failed my Standard 8 Examination for th...\n",
       "5262    When my cousin's wife refused him sex and clai...\n",
       "778     My father was a member of the public Alcohol a...\n",
       "5854    When my mother scolded me for something, very ...\n",
       "                              ...                        \n",
       "54                              Railway station good-bye.\n",
       "3238    My sister and I were fighting as usual. She's ...\n",
       "6263    The sadness came to me when I heard that my gi...\n",
       "5094            I was scared of water when I was a child.\n",
       "6596    When I heard that I had passed my end of first...\n",
       "Name: texts, Length: 5212, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r'dataset.txt', header=None, index=None, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('eda_dataset.txt', header = None, sep='\\t', names=['emotions', 'texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emotions'] = data['emotions'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotions</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shame</td>\n",
       "      <td>had boyfriend dated broken they i after friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shame</td>\n",
       "      <td>when i date stamp my acquaintance beau after t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shame</td>\n",
       "      <td>later when i dated my friends boyfriend upward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shame</td>\n",
       "      <td>when i dated my friends boyfriend after they h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>failed standard the first time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotions                                              texts\n",
       "0    shame  had boyfriend dated broken they i after friend...\n",
       "1    shame  when i date stamp my acquaintance beau after t...\n",
       "2    shame  later when i dated my friends boyfriend upward...\n",
       "3    shame  when i dated my friends boyfriend after they h...\n",
       "4  sadness                     failed standard the first time"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['shame', 'sadness', 'joy', 'fear', 'disgust', 'guilt', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['emotions'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'a': 2,\n",
       " 'the': 3,\n",
       " 'my': 4,\n",
       " 'to': 5,\n",
       " 'and': 6,\n",
       " 'was': 7,\n",
       " 'when': 8,\n",
       " 'of': 9,\n",
       " 'in': 10,\n",
       " 'had': 11,\n",
       " 'me': 12,\n",
       " 'that': 13,\n",
       " 'for': 14,\n",
       " 'with': 15,\n",
       " 'not': 16,\n",
       " 'it': 17,\n",
       " 'at': 18,\n",
       " 'on': 19,\n",
       " 'he': 20,\n",
       " 'very': 21,\n",
       " 'friend': 22,\n",
       " 'felt': 23,\n",
       " 'an': 24,\n",
       " 'she': 25,\n",
       " 'her': 26,\n",
       " 'we': 27,\n",
       " 'one': 28,\n",
       " 'about': 29,\n",
       " 'as': 30,\n",
       " 'this': 31,\n",
       " 'after': 32,\n",
       " 'from': 33,\n",
       " 'by': 34,\n",
       " 'time': 35,\n",
       " 'were': 36,\n",
       " 'did': 37,\n",
       " 'out': 38,\n",
       " 'but': 39,\n",
       " 'who': 40,\n",
       " 'him': 41,\n",
       " 'because': 42,\n",
       " 'his': 43,\n",
       " 'been': 44,\n",
       " 'got': 45,\n",
       " 'have': 46,\n",
       " 'which': 47,\n",
       " 'some': 48,\n",
       " 'home': 49,\n",
       " 'mother': 50,\n",
       " 'up': 51,\n",
       " 'friends': 52,\n",
       " 'told': 53,\n",
       " 'would': 54,\n",
       " 'so': 55,\n",
       " 'day': 56,\n",
       " 'they': 57,\n",
       " 'do': 58,\n",
       " 'school': 59,\n",
       " 'be': 60,\n",
       " 'people': 61,\n",
       " 'went': 62,\n",
       " 'there': 63,\n",
       " 'saw': 64,\n",
       " 'is': 65,\n",
       " 'person': 66,\n",
       " 'first': 67,\n",
       " 'our': 68,\n",
       " 'father': 69,\n",
       " 'came': 70,\n",
       " 'could': 71,\n",
       " 'all': 72,\n",
       " 'mine': 73,\n",
       " 'night': 74,\n",
       " 'go': 75,\n",
       " 'made': 76,\n",
       " \"didn't\": 77,\n",
       " 'girl': 78,\n",
       " 'having': 79,\n",
       " 'back': 80,\n",
       " 'car': 81,\n",
       " 'being': 82,\n",
       " 'other': 83,\n",
       " 'something': 84,\n",
       " 'boyfriend': 85,\n",
       " 'someone': 86,\n",
       " 'found': 87,\n",
       " 'them': 88,\n",
       " 'angry': 89,\n",
       " 'parents': 90,\n",
       " 'close': 91,\n",
       " 'good': 92,\n",
       " 'died': 93,\n",
       " 'while': 94,\n",
       " 'thought': 95,\n",
       " 'wanted': 96,\n",
       " 'then': 97,\n",
       " 'see': 98,\n",
       " 'man': 99,\n",
       " 'away': 100,\n",
       " 'another': 101,\n",
       " 'or': 102,\n",
       " 'brother': 103,\n",
       " 'sister': 104,\n",
       " 'feel': 105,\n",
       " 'asked': 106,\n",
       " 'years': 107,\n",
       " 'what': 108,\n",
       " 'get': 109,\n",
       " 'going': 110,\n",
       " 'work': 111,\n",
       " 'during': 112,\n",
       " 'no': 113,\n",
       " 'am': 114,\n",
       " 'passed': 115,\n",
       " 'know': 116,\n",
       " 'into': 117,\n",
       " 'year': 118,\n",
       " 'class': 119,\n",
       " 'guilty': 120,\n",
       " 'girlfriend': 121,\n",
       " 'before': 122,\n",
       " 'two': 123,\n",
       " 'ashamed': 124,\n",
       " 'without': 125,\n",
       " 'house': 126,\n",
       " 'sad': 127,\n",
       " 'way': 128,\n",
       " 'heard': 129,\n",
       " 'said': 130,\n",
       " 'myself': 131,\n",
       " 'exam': 132,\n",
       " 'much': 133,\n",
       " 'like': 134,\n",
       " 'left': 135,\n",
       " 'only': 136,\n",
       " 'just': 137,\n",
       " 'alone': 138,\n",
       " 'really': 139,\n",
       " 'us': 140,\n",
       " 'old': 141,\n",
       " 'later': 142,\n",
       " 'university': 143,\n",
       " 'last': 144,\n",
       " 'once': 145,\n",
       " 'family': 146,\n",
       " 'things': 147,\n",
       " 'well': 148,\n",
       " 'started': 149,\n",
       " 'long': 150,\n",
       " 'money': 151,\n",
       " 'disgusted': 152,\n",
       " 'few': 153,\n",
       " 'room': 154,\n",
       " 'place': 155,\n",
       " 'down': 156,\n",
       " 'feeling': 157,\n",
       " 'failed': 158,\n",
       " 'whom': 159,\n",
       " 'child': 160,\n",
       " 'over': 161,\n",
       " 'afraid': 162,\n",
       " 'love': 163,\n",
       " 'bad': 164,\n",
       " 'took': 165,\n",
       " 'boy': 166,\n",
       " 'take': 167,\n",
       " 'teacher': 168,\n",
       " 'late': 169,\n",
       " 'tried': 170,\n",
       " 'met': 171,\n",
       " 'done': 172,\n",
       " 'certain': 173,\n",
       " 'drunk': 174,\n",
       " 'fear': 175,\n",
       " 'off': 176,\n",
       " 'too': 177,\n",
       " 'are': 178,\n",
       " 'lost': 179,\n",
       " 'bus': 180,\n",
       " 'days': 181,\n",
       " 'happened': 182,\n",
       " 'come': 183,\n",
       " 'more': 184,\n",
       " 'relationship': 185,\n",
       " 'where': 186,\n",
       " 'if': 187,\n",
       " 'death': 188,\n",
       " 'life': 189,\n",
       " 'accident': 190,\n",
       " 'examination': 191,\n",
       " 'knew': 192,\n",
       " 'ago': 193,\n",
       " 'little': 194,\n",
       " 'any': 195,\n",
       " 'getting': 196,\n",
       " 'how': 197,\n",
       " 'several': 198,\n",
       " 'lot': 199,\n",
       " 'happy': 200,\n",
       " 'even': 201,\n",
       " 'exams': 202,\n",
       " 'caught': 203,\n",
       " 'realized': 204,\n",
       " 'best': 205,\n",
       " 'also': 206,\n",
       " 'situation': 207,\n",
       " 'party': 208,\n",
       " 'gave': 209,\n",
       " 'suddenly': 210,\n",
       " 'their': 211,\n",
       " 'doing': 212,\n",
       " 'always': 213,\n",
       " 'through': 214,\n",
       " 'front': 215,\n",
       " 'joy': 216,\n",
       " 'again': 217,\n",
       " 'talking': 218,\n",
       " 'months': 219,\n",
       " 'received': 220,\n",
       " 'same': 221,\n",
       " 'called': 222,\n",
       " 'than': 223,\n",
       " 'fell': 224,\n",
       " 'new': 225,\n",
       " 'reason': 226,\n",
       " 'many': 227,\n",
       " 'next': 228,\n",
       " 'quite': 229,\n",
       " 'help': 230,\n",
       " 'results': 231,\n",
       " 'give': 232,\n",
       " 'wrong': 233,\n",
       " 'never': 234,\n",
       " 'put': 235,\n",
       " 'young': 236,\n",
       " 'walking': 237,\n",
       " 'disgust': 238,\n",
       " 'still': 239,\n",
       " 'became': 240,\n",
       " 'woman': 241,\n",
       " 'able': 242,\n",
       " 'job': 243,\n",
       " 'promised': 244,\n",
       " 'group': 245,\n",
       " 'broke': 246,\n",
       " 'make': 247,\n",
       " 'hospital': 248,\n",
       " 'own': 249,\n",
       " 'ill': 250,\n",
       " 'you': 251,\n",
       " 'grandmother': 252,\n",
       " 'should': 253,\n",
       " 'week': 254,\n",
       " 'each': 255,\n",
       " 'telling': 256,\n",
       " 'leave': 257,\n",
       " 'study': 258,\n",
       " 'coming': 259,\n",
       " 'three': 260,\n",
       " 'behind': 261,\n",
       " 'dog': 262,\n",
       " 'morning': 263,\n",
       " 'letter': 264,\n",
       " 'student': 265,\n",
       " 'around': 266,\n",
       " 'shame': 267,\n",
       " 'others': 268,\n",
       " 'road': 269,\n",
       " 'however': 270,\n",
       " 'tell': 271,\n",
       " 'towards': 272,\n",
       " 'dark': 273,\n",
       " 'evening': 274,\n",
       " 'hurt': 275,\n",
       " 'against': 276,\n",
       " 'street': 277,\n",
       " 'accepted': 278,\n",
       " 'together': 279,\n",
       " 'almost': 280,\n",
       " 'read': 281,\n",
       " 'anger': 282,\n",
       " 'remember': 283,\n",
       " 'enough': 284,\n",
       " 'gone': 285,\n",
       " 'guilt': 286,\n",
       " 'seen': 287,\n",
       " \"couldn't\": 288,\n",
       " 'refused': 289,\n",
       " 'important': 290,\n",
       " 'end': 291,\n",
       " 'film': 292,\n",
       " 'decided': 293,\n",
       " 'present': 294,\n",
       " '2': 295,\n",
       " 'talk': 296,\n",
       " 'bed': 297,\n",
       " 'though': 298,\n",
       " 'think': 299,\n",
       " 'has': 300,\n",
       " \"friend's\": 301,\n",
       " 'door': 302,\n",
       " 'sitting': 303,\n",
       " 'feelings': 304,\n",
       " 'english': 305,\n",
       " 'right': 306,\n",
       " 'hours': 307,\n",
       " 'began': 308,\n",
       " 'find': 309,\n",
       " 'discovered': 310,\n",
       " 'now': 311,\n",
       " 'watching': 312,\n",
       " 'weeks': 313,\n",
       " 'leaving': 314,\n",
       " 'taken': 315,\n",
       " 'looking': 316,\n",
       " 'want': 317,\n",
       " 'students': 318,\n",
       " 'form': 319,\n",
       " 'great': 320,\n",
       " 'badly': 321,\n",
       " 'often': 322,\n",
       " 'let': 323,\n",
       " 'husband': 324,\n",
       " 'part': 325,\n",
       " 'loved': 326,\n",
       " 'book': 327,\n",
       " 'liked': 328,\n",
       " 'used': 329,\n",
       " 'expected': 330,\n",
       " 'turned': 331,\n",
       " 'meeting': 332,\n",
       " 'birthday': 333,\n",
       " 'driving': 334,\n",
       " 'visit': 335,\n",
       " 'grade': 336,\n",
       " 'holiday': 337,\n",
       " 'although': 338,\n",
       " 'somebody': 339,\n",
       " 'high': 340,\n",
       " 'call': 341,\n",
       " 'moment': 342,\n",
       " 'answer': 343,\n",
       " 'secondary': 344,\n",
       " 'live': 345,\n",
       " 'accused': 346,\n",
       " 'scared': 347,\n",
       " 'fact': 348,\n",
       " '3': 349,\n",
       " 'finding': 350,\n",
       " 'meet': 351,\n",
       " 'children': 352,\n",
       " 'anything': 353,\n",
       " 'looked': 354,\n",
       " 'ran': 355,\n",
       " 'college': 356,\n",
       " 'playing': 357,\n",
       " 'most': 358,\n",
       " 'given': 359,\n",
       " 'near': 360,\n",
       " 'trying': 361,\n",
       " 'hard': 362,\n",
       " 'caused': 363,\n",
       " 'whole': 364,\n",
       " 'result': 365,\n",
       " 'selected': 366,\n",
       " 'everything': 367,\n",
       " 'fight': 368,\n",
       " 'stop': 369,\n",
       " 'test': 370,\n",
       " 'lectures': 371,\n",
       " 'relative': 372,\n",
       " 'course': 373,\n",
       " 'summer': 374,\n",
       " 'guy': 375,\n",
       " 'between': 376,\n",
       " 'can': 377,\n",
       " 'seeing': 378,\n",
       " 'thing': 379,\n",
       " 'care': 380,\n",
       " 'nothing': 381,\n",
       " 'marks': 382,\n",
       " 'girls': 383,\n",
       " \"don't\": 384,\n",
       " 'such': 385,\n",
       " 'small': 386,\n",
       " 'waiting': 387,\n",
       " 'forgot': 388,\n",
       " 'argument': 389,\n",
       " 'insulted': 390,\n",
       " 'm': 391,\n",
       " 'stayed': 392,\n",
       " 'pass': 393,\n",
       " 'thinking': 394,\n",
       " 'rather': 395,\n",
       " 'due': 396,\n",
       " 'tv': 397,\n",
       " 'times': 398,\n",
       " 'every': 399,\n",
       " 'treated': 400,\n",
       " 'sadness': 401,\n",
       " 'food': 402,\n",
       " 'town': 403,\n",
       " 'grandfather': 404,\n",
       " 'working': 405,\n",
       " 'wife': 406,\n",
       " 'stay': 407,\n",
       " '5': 408,\n",
       " 'big': 409,\n",
       " 'face': 410,\n",
       " 'public': 411,\n",
       " 'lied': 412,\n",
       " 'driver': 413,\n",
       " 'saying': 414,\n",
       " 'team': 415,\n",
       " 'hit': 416,\n",
       " 'until': 417,\n",
       " \"wasn't\": 418,\n",
       " 'football': 419,\n",
       " 'full': 420,\n",
       " 'needed': 421,\n",
       " 'talked': 422,\n",
       " 'second': 423,\n",
       " 'kept': 424,\n",
       " 'involved': 425,\n",
       " 'behaviour': 426,\n",
       " 'cousin': 427,\n",
       " 'making': 428,\n",
       " 'son': 429,\n",
       " 'finished': 430,\n",
       " 'trip': 431,\n",
       " 'finally': 432,\n",
       " 'sick': 433,\n",
       " 'already': 434,\n",
       " 'spent': 435,\n",
       " 'uncle': 436,\n",
       " 'dead': 437,\n",
       " 'stolen': 438,\n",
       " 'else': 439,\n",
       " 'understand': 440,\n",
       " 'stole': 441,\n",
       " 'sleep': 442,\n",
       " 'taking': 443,\n",
       " 'under': 444,\n",
       " 'sexual': 445,\n",
       " 'etc': 446,\n",
       " '10': 447,\n",
       " 'instead': 448,\n",
       " 'relatives': 449,\n",
       " 'e': 450,\n",
       " 'studies': 451,\n",
       " 'along': 452,\n",
       " 'will': 453,\n",
       " 'problems': 454,\n",
       " 'arrived': 455,\n",
       " 'studying': 456,\n",
       " 'hour': 457,\n",
       " 'recently': 458,\n",
       " 'difficult': 459,\n",
       " 'head': 460,\n",
       " 'match': 461,\n",
       " 'lecture': 462,\n",
       " 'seemed': 463,\n",
       " 'news': 464,\n",
       " \"hadn't\": 465,\n",
       " 'younger': 466,\n",
       " 'admitted': 467,\n",
       " 'flat': 468,\n",
       " 'feared': 469,\n",
       " 'stopped': 470,\n",
       " 'himself': 471,\n",
       " 'water': 472,\n",
       " 'won': 473,\n",
       " 'say': 474,\n",
       " 'examinations': 475,\n",
       " 'since': 476,\n",
       " 'quarrel': 477,\n",
       " 'movie': 478,\n",
       " 'side': 479,\n",
       " 'entrance': 480,\n",
       " 'play': 481,\n",
       " 'dinner': 482,\n",
       " 'age': 483,\n",
       " 'prepared': 484,\n",
       " 'fellow': 485,\n",
       " 'pregnant': 486,\n",
       " '4': 487,\n",
       " 'scolded': 488,\n",
       " 'examn': 489,\n",
       " 'name': 490,\n",
       " 'your': 491,\n",
       " 't': 492,\n",
       " 'nearly': 493,\n",
       " 'frightened': 494,\n",
       " 'walk': 495,\n",
       " 'why': 496,\n",
       " 'boys': 497,\n",
       " 'buy': 498,\n",
       " 'baby': 499,\n",
       " 'turn': 500,\n",
       " 'crying': 501,\n",
       " 'b': 502,\n",
       " 'short': 503,\n",
       " 'forced': 504,\n",
       " 'negative': 505,\n",
       " 'middle': 506,\n",
       " 'police': 507,\n",
       " 'colleague': 508,\n",
       " 'outside': 509,\n",
       " 'phone': 510,\n",
       " 'better': 511,\n",
       " 'separated': 512,\n",
       " 'entered': 513,\n",
       " 'mate': 514,\n",
       " 'pay': 515,\n",
       " 'question': 516,\n",
       " 'classmate': 517,\n",
       " 'learnt': 518,\n",
       " 'missed': 519,\n",
       " 'eating': 520,\n",
       " 'daughter': 521,\n",
       " 'holidays': 522,\n",
       " 'experienced': 523,\n",
       " 'discussion': 524,\n",
       " 'eat': 525,\n",
       " 'train': 526,\n",
       " 'final': 527,\n",
       " 'lived': 528,\n",
       " 'homework': 529,\n",
       " 'use': 530,\n",
       " 'cancer': 531,\n",
       " 'clothes': 532,\n",
       " 'operation': 533,\n",
       " 'beaten': 534,\n",
       " 'floor': 535,\n",
       " 'event': 536,\n",
       " 'moved': 537,\n",
       " 'half': 538,\n",
       " 's': 539,\n",
       " '7': 540,\n",
       " 'attention': 541,\n",
       " 'china': 542,\n",
       " 'illness': 543,\n",
       " 'cannot': 544,\n",
       " 'worked': 545,\n",
       " 'shop': 546,\n",
       " 'subject': 547,\n",
       " 'these': 548,\n",
       " 'hand': 549,\n",
       " 'acquaintance': 550,\n",
       " 'emotion': 551,\n",
       " 'sometimes': 552,\n",
       " 'both': 553,\n",
       " 'cat': 554,\n",
       " 'especially': 555,\n",
       " 'broken': 556,\n",
       " 'sleeping': 557,\n",
       " 'appointment': 558,\n",
       " 'may': 559,\n",
       " 'hall': 560,\n",
       " 'married': 561,\n",
       " 'medical': 562,\n",
       " 'walked': 563,\n",
       " 'leader': 564,\n",
       " 'country': 565,\n",
       " 'supposed': 566,\n",
       " 'responsible': 567,\n",
       " 'terrible': 568,\n",
       " 'chosen': 569,\n",
       " 'patient': 570,\n",
       " \"father's\": 571,\n",
       " 'living': 572,\n",
       " '8': 573,\n",
       " 'everyone': 574,\n",
       " 'radio': 575,\n",
       " 'does': 576,\n",
       " 'terribly': 577,\n",
       " 'month': 578,\n",
       " 'men': 579,\n",
       " 'knowing': 580,\n",
       " 'ask': 581,\n",
       " 'dirty': 582,\n",
       " 'primary': 583,\n",
       " 'heart': 584,\n",
       " 'fault': 585,\n",
       " 'behaved': 586,\n",
       " 'c': 587,\n",
       " 'paper': 588,\n",
       " \"i'm\": 589,\n",
       " 'usually': 590,\n",
       " 'threatened': 591,\n",
       " 'might': 592,\n",
       " 'v': 593,\n",
       " 'lying': 594,\n",
       " 'particular': 595,\n",
       " 'showed': 596,\n",
       " 'keep': 597,\n",
       " 'cut': 598,\n",
       " 'despite': 599,\n",
       " 'seat': 600,\n",
       " 'concerning': 601,\n",
       " 'soon': 602,\n",
       " 'hearing': 603,\n",
       " \"mother's\": 604,\n",
       " 'appeared': 605,\n",
       " 'run': 606,\n",
       " 'conversation': 607,\n",
       " 'abroad': 608,\n",
       " 'returned': 609,\n",
       " '6': 610,\n",
       " 'nice': 611,\n",
       " 'borrowed': 612,\n",
       " 'standard': 613,\n",
       " 'questions': 614,\n",
       " 'words': 615,\n",
       " 'classmates': 616,\n",
       " 'low': 617,\n",
       " 'try': 618,\n",
       " 'brought': 619,\n",
       " 'restaurant': 620,\n",
       " 'members': 621,\n",
       " 'personal': 622,\n",
       " 'period': 623,\n",
       " 'window': 624,\n",
       " 'christmas': 625,\n",
       " 'bought': 626,\n",
       " 'game': 627,\n",
       " 'roommate': 628,\n",
       " 'unza': 629,\n",
       " 'experience': 630,\n",
       " 'stupid': 631,\n",
       " 'elder': 632,\n",
       " 'till': 633,\n",
       " 'body': 634,\n",
       " 'sent': 635,\n",
       " 'reasons': 636,\n",
       " 'woke': 637,\n",
       " 'using': 638,\n",
       " 'look': 639,\n",
       " 'glad': 640,\n",
       " 'write': 641,\n",
       " 'following': 642,\n",
       " \"wouldn't\": 643,\n",
       " '1': 644,\n",
       " 'opinion': 645,\n",
       " 'invited': 646,\n",
       " 'midterm': 647,\n",
       " 'actually': 648,\n",
       " 'weekend': 649,\n",
       " 'sense': 650,\n",
       " 'knocked': 651,\n",
       " 'large': 652,\n",
       " 'hear': 653,\n",
       " 'newspaper': 654,\n",
       " 'studied': 655,\n",
       " 'different': 656,\n",
       " 'completely': 657,\n",
       " 'aunt': 658,\n",
       " 'lack': 659,\n",
       " 'stealing': 660,\n",
       " 'social': 661,\n",
       " 'sure': 662,\n",
       " 'four': 663,\n",
       " 'show': 664,\n",
       " 'funeral': 665,\n",
       " 'move': 666,\n",
       " 'serious': 667,\n",
       " 'born': 668,\n",
       " 'ended': 669,\n",
       " 'lunch': 670,\n",
       " 'earlier': 671,\n",
       " 'early': 672,\n",
       " 'written': 673,\n",
       " 'killed': 674,\n",
       " 'reading': 675,\n",
       " 'disgusting': 676,\n",
       " 'believe': 677,\n",
       " 'law': 678,\n",
       " 'third': 679,\n",
       " 'date': 680,\n",
       " 'table': 681,\n",
       " 'noise': 682,\n",
       " 'happen': 683,\n",
       " 'learned': 684,\n",
       " 'those': 685,\n",
       " \"i'd\": 686,\n",
       " 'lie': 687,\n",
       " 'staying': 688,\n",
       " 'contact': 689,\n",
       " 'self': 690,\n",
       " 'shouted': 691,\n",
       " 'sat': 692,\n",
       " 'lonely': 693,\n",
       " 'world': 694,\n",
       " 'nobody': 695,\n",
       " 'scene': 696,\n",
       " 'attitude': 697,\n",
       " 'asking': 698,\n",
       " 'boat': 699,\n",
       " 'travelling': 700,\n",
       " 'past': 701,\n",
       " 'afterwards': 702,\n",
       " 'thus': 703,\n",
       " 'wrote': 704,\n",
       " 'wedding': 705,\n",
       " 'mark': 706,\n",
       " 'lady': 707,\n",
       " 'story': 708,\n",
       " 'real': 709,\n",
       " 'giving': 710,\n",
       " 'laughed': 711,\n",
       " 'beat': 712,\n",
       " 'rude': 713,\n",
       " 'professor': 714,\n",
       " 'unjustly': 715,\n",
       " 'considered': 716,\n",
       " 'passing': 717,\n",
       " 'break': 718,\n",
       " 'station': 719,\n",
       " 'dad': 720,\n",
       " 'list': 721,\n",
       " 'planned': 722,\n",
       " 'promise': 723,\n",
       " 'birth': 724,\n",
       " 'term': 725,\n",
       " \"can't\": 726,\n",
       " 'mistake': 727,\n",
       " 'couple': 728,\n",
       " 'manner': 729,\n",
       " 'known': 730,\n",
       " 'report': 731,\n",
       " 'point': 732,\n",
       " 'unable': 733,\n",
       " 'open': 734,\n",
       " 'here': 735,\n",
       " 'opened': 736,\n",
       " 'doctor': 737,\n",
       " 'strong': 738,\n",
       " 'order': 739,\n",
       " 'across': 740,\n",
       " 'winning': 741,\n",
       " 'city': 742,\n",
       " 'busy': 743,\n",
       " 'partner': 744,\n",
       " 'far': 745,\n",
       " 'noticed': 746,\n",
       " 'alcohol': 747,\n",
       " 'poor': 748,\n",
       " 'strange': 749,\n",
       " 'therefore': 750,\n",
       " 'played': 751,\n",
       " 'unexpectedly': 752,\n",
       " 'speak': 753,\n",
       " 'mind': 754,\n",
       " 'language': 755,\n",
       " 'incident': 756,\n",
       " 'horror': 757,\n",
       " 'forgotten': 758,\n",
       " 'ex': 759,\n",
       " 'cold': 760,\n",
       " 'education': 761,\n",
       " 'slept': 762,\n",
       " 'respect': 763,\n",
       " 'set': 764,\n",
       " 'threw': 765,\n",
       " 'company': 766,\n",
       " 'moving': 767,\n",
       " 'p': 768,\n",
       " 'minutes': 769,\n",
       " 'drinking': 770,\n",
       " 'allowed': 771,\n",
       " 'punished': 772,\n",
       " 'goal': 773,\n",
       " 'attacked': 774,\n",
       " 'understood': 775,\n",
       " 'books': 776,\n",
       " 'everybody': 777,\n",
       " 'depressed': 778,\n",
       " 'unexpected': 779,\n",
       " 'longer': 780,\n",
       " 'smell': 781,\n",
       " 'succeed': 782,\n",
       " 'change': 783,\n",
       " 'drink': 784,\n",
       " 'library': 785,\n",
       " 'spoke': 786,\n",
       " 'older': 787,\n",
       " 'accept': 788,\n",
       " 'standing': 789,\n",
       " 'running': 790,\n",
       " 'seriously': 791,\n",
       " 'beach': 792,\n",
       " 'whenever': 793,\n",
       " 'glass': 794,\n",
       " 'watch': 795,\n",
       " 'suffered': 796,\n",
       " 'sorry': 797,\n",
       " '12': 798,\n",
       " \"sister's\": 799,\n",
       " 'herself': 800,\n",
       " 'army': 801,\n",
       " 'blamed': 802,\n",
       " 'die': 803,\n",
       " 'sex': 804,\n",
       " 'example': 805,\n",
       " 'act': 806,\n",
       " 'ticket': 807,\n",
       " 'unfortunately': 808,\n",
       " 'yet': 809,\n",
       " 'certificate': 810,\n",
       " 'blame': 811,\n",
       " 'arranged': 812,\n",
       " 'traffic': 813,\n",
       " 'pick': 814,\n",
       " 'acting': 815,\n",
       " 'drove': 816,\n",
       " 'makes': 817,\n",
       " 'consequences': 818,\n",
       " 'reacted': 819,\n",
       " 'totally': 820,\n",
       " 'possible': 821,\n",
       " 'music': 822,\n",
       " 'service': 823,\n",
       " 'its': 824,\n",
       " 'ate': 825,\n",
       " 'start': 826,\n",
       " 'waited': 827,\n",
       " 'watched': 828,\n",
       " 'lies': 829,\n",
       " 'disappointed': 830,\n",
       " 'persons': 831,\n",
       " 'control': 832,\n",
       " 'visited': 833,\n",
       " 'cried': 834,\n",
       " 'dying': 835,\n",
       " 'fire': 836,\n",
       " 'behavior': 837,\n",
       " 'admission': 838,\n",
       " 'higher': 839,\n",
       " 'ball': 840,\n",
       " 'number': 841,\n",
       " 'deliberately': 842,\n",
       " 'chinese': 843,\n",
       " 'drive': 844,\n",
       " 'war': 845,\n",
       " 'forest': 846,\n",
       " 'dropped': 847,\n",
       " 'south': 848,\n",
       " 'kind': 849,\n",
       " 'neighbour': 850,\n",
       " 'toilet': 851,\n",
       " 'informed': 852,\n",
       " 'listen': 853,\n",
       " 'share': 854,\n",
       " 'position': 855,\n",
       " 'area': 856,\n",
       " 'chair': 857,\n",
       " 'light': 858,\n",
       " 'whose': 859,\n",
       " 'fighting': 860,\n",
       " 'above': 861,\n",
       " 'picked': 862,\n",
       " 'lover': 863,\n",
       " 'agreed': 864,\n",
       " 'beating': 865,\n",
       " 'followed': 866,\n",
       " 'swimming': 867,\n",
       " 'stories': 868,\n",
       " 'stand': 869,\n",
       " 'temper': 870,\n",
       " 'concert': 871,\n",
       " 'village': 872,\n",
       " 'dogs': 873,\n",
       " 'loss': 874,\n",
       " 'camp': 875,\n",
       " 'kitchen': 876,\n",
       " 'meant': 877,\n",
       " 'listening': 878,\n",
       " 'bike': 879,\n",
       " 'ever': 880,\n",
       " 'future': 881,\n",
       " 'speaking': 882,\n",
       " 'injured': 883,\n",
       " 'five': 884,\n",
       " 'colleagues': 885,\n",
       " '20': 886,\n",
       " 'immediately': 887,\n",
       " 'psychology': 888,\n",
       " 'sports': 889,\n",
       " 'lesson': 890,\n",
       " 'midnight': 891,\n",
       " 'bag': 892,\n",
       " 'successful': 893,\n",
       " 'trousers': 894,\n",
       " 'male': 895,\n",
       " 'true': 896,\n",
       " 'committed': 897,\n",
       " 'mum': 898,\n",
       " 'afternoon': 899,\n",
       " 'occurred': 900,\n",
       " 'sisters': 901,\n",
       " 'unfair': 902,\n",
       " 'lives': 903,\n",
       " 'u': 904,\n",
       " 'losing': 905,\n",
       " 'blood': 906,\n",
       " 'church': 907,\n",
       " 'bar': 908,\n",
       " 'quiet': 909,\n",
       " 'finish': 910,\n",
       " 'red': 911,\n",
       " 'medicine': 912,\n",
       " 'training': 913,\n",
       " 'presence': 914,\n",
       " 'unjust': 915,\n",
       " 'failing': 916,\n",
       " 'leg': 917,\n",
       " 'wait': 918,\n",
       " 'support': 919,\n",
       " \"year's\": 920,\n",
       " 'eve': 921,\n",
       " 'changed': 922,\n",
       " 'extremely': 923,\n",
       " 'deep': 924,\n",
       " 'lecturer': 925,\n",
       " 'former': 926,\n",
       " 'worried': 927,\n",
       " 'program': 928,\n",
       " 'cross': 929,\n",
       " 'interested': 930,\n",
       " 'disappeared': 931,\n",
       " 'word': 932,\n",
       " 'misunderstood': 933,\n",
       " 'pain': 934,\n",
       " 'hostel': 935,\n",
       " 'ten': 936,\n",
       " 'worst': 937,\n",
       " 'bicycle': 938,\n",
       " 'matter': 939,\n",
       " 'assistant': 940,\n",
       " 'shopping': 941,\n",
       " 'attend': 942,\n",
       " 'six': 943,\n",
       " 'unknown': 944,\n",
       " 'shoes': 945,\n",
       " 'beside': 946,\n",
       " 'stranger': 947,\n",
       " 'held': 948,\n",
       " 'visiting': 949,\n",
       " 'phoned': 950,\n",
       " 'fail': 951,\n",
       " 'classes': 952,\n",
       " 'level': 953,\n",
       " 'helped': 954,\n",
       " 'cup': 955,\n",
       " 'hardly': 956,\n",
       " 'relation': 957,\n",
       " 'must': 958,\n",
       " 'foreign': 959,\n",
       " 'enter': 960,\n",
       " 'spend': 961,\n",
       " 'niece': 962,\n",
       " 'excited': 963,\n",
       " 'dining': 964,\n",
       " 'possibility': 965,\n",
       " 'g': 966,\n",
       " 'locked': 967,\n",
       " 'closest': 968,\n",
       " 'god': 969,\n",
       " 'marriage': 970,\n",
       " 'truth': 971,\n",
       " 'intercourse': 972,\n",
       " 'unfairly': 973,\n",
       " 'breaking': 974,\n",
       " 'human': 975,\n",
       " 'graduation': 976,\n",
       " 'fall': 977,\n",
       " 'anyone': 978,\n",
       " 'confidence': 979,\n",
       " 'problem': 980,\n",
       " 'boss': 981,\n",
       " 'assignment': 982,\n",
       " 'simply': 983,\n",
       " 'animals': 984,\n",
       " 'acted': 985,\n",
       " 'top': 986,\n",
       " 'saturday': 987,\n",
       " 'telephone': 988,\n",
       " 'yesterday': 989,\n",
       " \"brother's\": 990,\n",
       " 'eyes': 991,\n",
       " 'fallen': 992,\n",
       " 'need': 993,\n",
       " 'brothers': 994,\n",
       " 'knowledge': 995,\n",
       " 'special': 996,\n",
       " 'receiving': 997,\n",
       " 'cared': 998,\n",
       " 'betrayed': 999,\n",
       " 'teachers': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open((os.path.join(DATAPATH, 'glove.6B.100d.txt')), encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "          \n",
    "f.close()\n",
    "          \n",
    "\n",
    "embedding_matrix = np.random.random((len(tokenizer.word_index) + 1, 100))\n",
    "          \n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers, initializers, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(100,), dtype='int32'))\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1,\n",
    "    100,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length = 100,\n",
    "    trainable=False,\n",
    "    name = 'embeddings'))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(50, activation='relu', ))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(17, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(tf.keras.layers.Embedding(100, 128))\n",
    "# model.add(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def _tokenize(text):\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens if not w.lower() in stop_words]\n",
    "    lems = []\n",
    "    for item in tokens:\n",
    "        lems.append(lemmatizer.lemmatize(item))\n",
    "    return ' '.join(lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenize('apple is very tasty.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_texts'] = df['texts'].apply(_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_texts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokenizer = Tokenizer()\n",
    "clean_tokenizer.fit_on_texts(df['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenied_data = clean_tokenizer.texts_to_sequences(df['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenied_data = pad_sequences(tokenied_data, padding='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenied_data[:6000]\n",
    "X_test = tokenied_data[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "encoder = MultiLabelBinarizer()\n",
    "labels = encoder.fit_transform(df['emotions'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.get_dummies(df['emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = labels[:6000]\n",
    "y_test = labels[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "94/94 [==============================] - 77s 814ms/step - loss: 0.5339 - accuracy: 0.2588 - val_loss: 0.5050 - val_accuracy: 0.2863\n",
      "Epoch 2/100\n",
      "94/94 [==============================] - 75s 799ms/step - loss: 0.5032 - accuracy: 0.2543 - val_loss: 0.4857 - val_accuracy: 0.4004\n",
      "Epoch 3/100\n",
      "94/94 [==============================] - 76s 803ms/step - loss: 0.4843 - accuracy: 0.2570 - val_loss: 0.4810 - val_accuracy: 0.2490\n",
      "Epoch 4/100\n",
      "94/94 [==============================] - 76s 804ms/step - loss: 0.4706 - accuracy: 0.2545 - val_loss: 0.4658 - val_accuracy: 0.4011\n",
      "Epoch 5/100\n",
      "94/94 [==============================] - 76s 813ms/step - loss: 0.4586 - accuracy: 0.2680 - val_loss: 0.4470 - val_accuracy: 0.4744\n",
      "Epoch 6/100\n",
      "94/94 [==============================] - 80s 854ms/step - loss: 0.4442 - accuracy: 0.2538 - val_loss: 0.4401 - val_accuracy: 0.2441\n",
      "Epoch 7/100\n",
      "94/94 [==============================] - 79s 836ms/step - loss: 0.4362 - accuracy: 0.2553 - val_loss: 0.4419 - val_accuracy: 0.1784\n",
      "Epoch 8/100\n",
      "94/94 [==============================] - 78s 834ms/step - loss: 0.4310 - accuracy: 0.2503 - val_loss: 0.4256 - val_accuracy: 0.2268\n",
      "Epoch 9/100\n",
      "94/94 [==============================] - 77s 822ms/step - loss: 0.4133 - accuracy: 0.2550 - val_loss: 0.4140 - val_accuracy: 0.3402\n",
      "Epoch 10/100\n",
      "94/94 [==============================] - 78s 826ms/step - loss: 0.4044 - accuracy: 0.2560 - val_loss: 0.4135 - val_accuracy: 0.1535\n",
      "Epoch 11/100\n",
      "94/94 [==============================] - 79s 838ms/step - loss: 0.4107 - accuracy: 0.2490 - val_loss: 0.4052 - val_accuracy: 0.3686\n",
      "Epoch 12/100\n",
      "94/94 [==============================] - 79s 843ms/step - loss: 0.3939 - accuracy: 0.2818 - val_loss: 0.3865 - val_accuracy: 0.2420\n",
      "Epoch 13/100\n",
      "94/94 [==============================] - 77s 820ms/step - loss: 0.3822 - accuracy: 0.2773 - val_loss: 0.3793 - val_accuracy: 0.4122\n",
      "Epoch 14/100\n",
      "94/94 [==============================] - 77s 824ms/step - loss: 0.3742 - accuracy: 0.2850 - val_loss: 0.3694 - val_accuracy: 0.3942\n",
      "Epoch 15/100\n",
      "94/94 [==============================] - 77s 817ms/step - loss: 0.3644 - accuracy: 0.2713 - val_loss: 0.3805 - val_accuracy: 0.2420\n",
      "Epoch 16/100\n",
      "94/94 [==============================] - 78s 832ms/step - loss: 0.3606 - accuracy: 0.2645 - val_loss: 0.3596 - val_accuracy: 0.2842\n",
      "Epoch 17/100\n",
      "94/94 [==============================] - 77s 820ms/step - loss: 0.3489 - accuracy: 0.2863 - val_loss: 0.3592 - val_accuracy: 0.3368\n",
      "Epoch 18/100\n",
      "94/94 [==============================] - 77s 816ms/step - loss: 0.3410 - accuracy: 0.2840 - val_loss: 0.3661 - val_accuracy: 0.3250\n",
      "Epoch 19/100\n",
      "94/94 [==============================] - 77s 824ms/step - loss: 0.3378 - accuracy: 0.2808 - val_loss: 0.3666 - val_accuracy: 0.3568\n",
      "Epoch 20/100\n",
      "94/94 [==============================] - 78s 835ms/step - loss: 0.3333 - accuracy: 0.2848 - val_loss: 0.3722 - val_accuracy: 0.2082\n",
      "Epoch 21/100\n",
      "94/94 [==============================] - 77s 819ms/step - loss: 0.3262 - accuracy: 0.2785 - val_loss: 0.3546 - val_accuracy: 0.3907\n",
      "Epoch 22/100\n",
      "94/94 [==============================] - 77s 815ms/step - loss: 0.3186 - accuracy: 0.2840 - val_loss: 0.3640 - val_accuracy: 0.3900\n",
      "Epoch 23/100\n",
      "94/94 [==============================] - 76s 813ms/step - loss: 0.3196 - accuracy: 0.2845 - val_loss: 0.3652 - val_accuracy: 0.3299\n",
      "Epoch 24/100\n",
      "94/94 [==============================] - 77s 824ms/step - loss: 0.3119 - accuracy: 0.2918 - val_loss: 0.3608 - val_accuracy: 0.2393\n",
      "Epoch 25/100\n",
      "94/94 [==============================] - 76s 808ms/step - loss: 0.3033 - accuracy: 0.3025 - val_loss: 0.3646 - val_accuracy: 0.3562\n",
      "Epoch 26/100\n",
      "94/94 [==============================] - 75s 796ms/step - loss: 0.3022 - accuracy: 0.3218 - val_loss: 0.3475 - val_accuracy: 0.2863\n",
      "Epoch 27/100\n",
      "94/94 [==============================] - 75s 793ms/step - loss: 0.2953 - accuracy: 0.2985 - val_loss: 0.3695 - val_accuracy: 0.2303\n",
      "Epoch 28/100\n",
      "94/94 [==============================] - 76s 809ms/step - loss: 0.2861 - accuracy: 0.2967 - val_loss: 0.3766 - val_accuracy: 0.1957\n",
      "Epoch 29/100\n",
      "94/94 [==============================] - 76s 811ms/step - loss: 0.2857 - accuracy: 0.2960 - val_loss: 0.3686 - val_accuracy: 0.2241\n",
      "Epoch 30/100\n",
      "94/94 [==============================] - 76s 803ms/step - loss: 0.2762 - accuracy: 0.3118 - val_loss: 0.3805 - val_accuracy: 0.3347\n",
      "Epoch 31/100\n",
      "94/94 [==============================] - 76s 812ms/step - loss: 0.2786 - accuracy: 0.2923 - val_loss: 0.3625 - val_accuracy: 0.2552\n",
      "Epoch 32/100\n",
      "94/94 [==============================] - 80s 852ms/step - loss: 0.2714 - accuracy: 0.2897 - val_loss: 0.3711 - val_accuracy: 0.2089\n",
      "Epoch 33/100\n",
      "94/94 [==============================] - 76s 806ms/step - loss: 0.2641 - accuracy: 0.2825 - val_loss: 0.3853 - val_accuracy: 0.1708\n",
      "Epoch 34/100\n",
      "94/94 [==============================] - 76s 807ms/step - loss: 0.2621 - accuracy: 0.2988 - val_loss: 0.3736 - val_accuracy: 0.3008\n",
      "Epoch 35/100\n",
      "94/94 [==============================] - 76s 813ms/step - loss: 0.2536 - accuracy: 0.3082 - val_loss: 0.3918 - val_accuracy: 0.2649\n",
      "Epoch 36/100\n",
      "94/94 [==============================] - 76s 807ms/step - loss: 0.2547 - accuracy: 0.3047 - val_loss: 0.3781 - val_accuracy: 0.4308\n",
      "Epoch 37/100\n",
      "94/94 [==============================] - 76s 808ms/step - loss: 0.2486 - accuracy: 0.3517 - val_loss: 0.3893 - val_accuracy: 0.3437\n",
      "Epoch 38/100\n",
      "94/94 [==============================] - 76s 813ms/step - loss: 0.2416 - accuracy: 0.2982 - val_loss: 0.4102 - val_accuracy: 0.1694\n",
      "Epoch 39/100\n",
      "94/94 [==============================] - 78s 825ms/step - loss: 0.2396 - accuracy: 0.3127 - val_loss: 0.3844 - val_accuracy: 0.3105\n",
      "Epoch 40/100\n",
      "94/94 [==============================] - 77s 816ms/step - loss: 0.2360 - accuracy: 0.2898 - val_loss: 0.3849 - val_accuracy: 0.1515\n",
      "Epoch 41/100\n",
      "94/94 [==============================] - 77s 822ms/step - loss: 0.2334 - accuracy: 0.2758 - val_loss: 0.3886 - val_accuracy: 0.2690\n",
      "Epoch 42/100\n",
      "94/94 [==============================] - 77s 816ms/step - loss: 0.2261 - accuracy: 0.3248 - val_loss: 0.3808 - val_accuracy: 0.4378\n",
      "Epoch 43/100\n",
      "94/94 [==============================] - 77s 814ms/step - loss: 0.2234 - accuracy: 0.3100 - val_loss: 0.3929 - val_accuracy: 0.2227\n",
      "Epoch 44/100\n",
      "94/94 [==============================] - 78s 830ms/step - loss: 0.2202 - accuracy: 0.3098 - val_loss: 0.4192 - val_accuracy: 0.3485\n",
      "Epoch 45/100\n",
      "94/94 [==============================] - 76s 810ms/step - loss: 0.2211 - accuracy: 0.3250 - val_loss: 0.4130 - val_accuracy: 0.3582\n",
      "Epoch 46/100\n",
      "94/94 [==============================] - 76s 811ms/step - loss: 0.2109 - accuracy: 0.2972 - val_loss: 0.4095 - val_accuracy: 0.2158\n",
      "Epoch 47/100\n",
      "94/94 [==============================] - 78s 832ms/step - loss: 0.2112 - accuracy: 0.3087 - val_loss: 0.4487 - val_accuracy: 0.4993\n",
      "Epoch 48/100\n",
      "94/94 [==============================] - 77s 824ms/step - loss: 0.2141 - accuracy: 0.3082 - val_loss: 0.4075 - val_accuracy: 0.2773\n",
      "Epoch 49/100\n",
      "94/94 [==============================] - 78s 827ms/step - loss: 0.2087 - accuracy: 0.3097 - val_loss: 0.4114 - val_accuracy: 0.3029\n",
      "Epoch 50/100\n",
      "94/94 [==============================] - 77s 821ms/step - loss: 0.2070 - accuracy: 0.3303 - val_loss: 0.4195 - val_accuracy: 0.2296\n",
      "Epoch 51/100\n",
      "94/94 [==============================] - 78s 828ms/step - loss: 0.1965 - accuracy: 0.3077 - val_loss: 0.4220 - val_accuracy: 0.1853\n",
      "Epoch 52/100\n",
      "94/94 [==============================] - 79s 841ms/step - loss: 0.1939 - accuracy: 0.3050 - val_loss: 0.4298 - val_accuracy: 0.4391\n",
      "Epoch 53/100\n",
      "94/94 [==============================] - 78s 825ms/step - loss: 0.1890 - accuracy: 0.3095 - val_loss: 0.4492 - val_accuracy: 0.4544\n",
      "Epoch 54/100\n",
      "94/94 [==============================] - 76s 811ms/step - loss: 0.1911 - accuracy: 0.3330 - val_loss: 0.4298 - val_accuracy: 0.2351\n",
      "Epoch 55/100\n",
      "94/94 [==============================] - 78s 831ms/step - loss: 0.1856 - accuracy: 0.3268 - val_loss: 0.4161 - val_accuracy: 0.4793\n",
      "Epoch 56/100\n",
      "94/94 [==============================] - 77s 818ms/step - loss: 0.1859 - accuracy: 0.3402 - val_loss: 0.4272 - val_accuracy: 0.4599\n",
      "Epoch 57/100\n",
      "79/94 [========================>.....] - ETA: 12s - loss: 0.1730 - accuracy: 0.3400"
     ]
    }
   ],
   "source": [
    "# history = model.fit(X_train, y_train, epochs = 100, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([\"this very long comment is not toxic\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tok.texts_to_sequences([\"this comment is not toxic\"])) \n",
    "print(tok.texts_to_sequences([\"this very long comment is not toxic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tok.texts_to_matrix([\"this comment is not toxic\"])) \n",
    "print(tok.texts_to_matrix([\"this very long comment is not toxic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max(tokenied_data, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenied_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "encoder = MultiLabelBinarizer()\n",
    "labels = encoder.fit_transform(df['emotions'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotions'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label = np.load(\n",
    "        os.path.join(CHECKPOINT_PATH, \"frozen_data/SKLEARN-data-7000.npy\"), allow_pickle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5584, 7000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, random_state=32)\n",
    "clf = MultinomialNB()\n",
    "\n",
    "param_grid = {'alpha': np.arange(1, 10, 1), 'fit_prior': [True, False]}\n",
    "\n",
    "gscv = GridSearchCV(clf, cv=folds, param_grid=param_grid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5580229226361032"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1, 'fit_prior': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5631870061905447"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "f1_score(preds, test_label, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5644468313641245"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preds, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(clf.classes_, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['i love you' 'i  you' '  ' ... 'i love ' 'i  you' ' love you'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4e3adab942bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m exp = explainer.explain_instance(\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;34m\"i love you\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    413\u001b[0m         data, yss, distances = self.__data_labels_distances(\n\u001b[0;32m    414\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             distance_metric=distance_metric)\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[1;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \"\"\"\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_log_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict_log_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;31m# normalize by P(x) = P(f_1, ..., f_n)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mlog_prob_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"classes_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0;32m    738\u001b[0m                 self.class_log_prior_)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['i love you' 'i  you' '  ' ... 'i love ' 'i  you' ' love you'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=np.reshape(clf.classes_, (-1, 1)))\n",
    "exp = explainer.explain_instance(\n",
    "    \"i love you\", clf.predict_proba, num_features=7\n",
    ")\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
